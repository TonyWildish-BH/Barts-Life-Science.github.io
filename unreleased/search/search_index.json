{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Barts Health Data Platform Documentation","text":""},{"location":"#overview","title":"Overview","text":"<p>Our documentation is spread across the three main functions within the Barts Health Data Platform (BHDP). These are: - The Data Portal for making Data Access Requests and managing your project once approved. - The Analytics Data Core which transforms the data that you will be authorised access to. - Our Secure Data Environment based in Azure where you will be able to analyse the data we provide.</p> <p>If you have issues with any of these services or documentation then please contact XXX</p>"},{"location":"sde/Home/","title":"The Secure Data Environment","text":"<p>This page contains the user documentation for the Barts Health Secure Data Environment (SDE).</p>"},{"location":"sde/Home/#introduction","title":"Introduction","text":"<p>A note on terminology: We refer to a Secure Data Environment (SDE) and a Trusted Research Environment (TRE) interchangeably here. Locally, we prefer the SDE designation, but we're based on a package which calls itself a TRE. To all intents and purposes, they mean the same thing.</p>"},{"location":"sde/Home/#what-is-a-secure-data-environment","title":"What is a Secure Data Environment?","text":"<p>As the name implies, an SDE is an environment in which data can be analysed securely. This means that there is no direct access to the internet, data cannot be uploaded or downloaded freely by the User. This greatly reduces the possibility of data leaks, accidental or otherwise, and is a requirement for handling sensitive data.</p> <p>Data can only be put into or extracted from the SDE by one of two routes: * Data from Barts Health requested and approved via the Data Access Committee (DAC) will be pushed into the project storage after any appropriate pre-processing, such as anonymisation. With the approval of the DAC, we can link data sets from other Data Controllers to our own data either anonymised or psuedoanonymised, depending on the requirements. * Other data, software or tools can be imported or exported by going through an 'airlock' process, where the data is staged through secure storage and is reviewed by the PM team before access is granted and the material is available the other side of the airlock.</p> <p>In order to keep the data secure, there are some restrictions on the analysis environment. The data can be accessed through virtual machines (VMs), contained in a workspace. Each workspace is unique to a project, with only project-members having access to it. The VMs in a workspace cannot be accessed directly through Secure Shell (SSH), they have to be accessed by a web-based virtual desktop.</p> <p>Our SDE is based on the AzureTRE package from Microsoft. This is an implementation built by Microsoft as an 'accelerator' for the community. It's not a product in the sense that you can't click on a button in the Azure portal and have it deployed, and they don't officially support it at this time. For this reason, we have to develop it further ourselves, so that it satisfies all our requirements. See the timeline below for more details.</p>"},{"location":"sde/Home/#project-workspaces","title":"Project workspaces","text":"<p>A workspace is a container for resources, or 'services' for a project. Services include things like a Git mirror based on Gitea, a Nexus repository which allows you to access software repositories such as CRAN and PyPi, and a Guacamole Virtual Desktop service, which allows you to create virtual machines. More detail on each service is available elsewhere (TBD).</p> <p>Once a project has been approved through the DAC, we will provision a workspace for it. Users will be assigned one of two roles: * Workspace Owner, who can create and destroy resources in the workspace, or  * Workspace Researcher, who can use resources, but can only create or destroy selected resources, such as VMs.</p> <p>A Workspace Owner can manage all the resources created by any researcher, but a researcher can only manage resources that they themselves create.</p> <p>You should then be able to log into the SDE, select your workspace, and view/manage/use the resources in your project.</p>"},{"location":"sde/Home/#getting-help","title":"Getting help","text":"<p>We have further information on how you can provide feedback and get support.</p>"},{"location":"sde/Home/#using-your-workspace","title":"Using your workspace","text":"<p>See Using your workspace</p>"},{"location":"sde/Home/#timeline","title":"Timeline","text":"<p>The approximate timeline for the SDE development is as follows: * The Alpha version is already available (June 2024). The Alpha is not yet verified to be secure for sensitive data and is only open to select users. We will use feedback from those users to drive the development plan going forwards. As such, we don't expect the users of the alpha to achieve significant analysis results. * A Beta will be released in the near future, as soon as we're satisfied the SDE is usable for sensitive data. At this point, real analysis work should be possible. * By autumn 2024, we expect to release an MVP, or Minimum Viable Product. This means the basic functionality is there for most or all types of analysis, and the platform is ready for a broader range of users. * In Spring 2025, we expect to release a full production version, with the features and functionality we need to support the full range of analysis activities. * Further development and enhancements will continue throughout 2025, at least.</p>"},{"location":"sde/Using-your-workspace/","title":"Using your workspace","text":"<p>You access your workspace via the web, once you've been assigned an appropriate role. A workspace consists of several workspace services, plus also access to shared services, which are shared by all the workspaces in the SDE. The Workspace Owner can manage all the resources in their own workspaces, but cannot manipulate the shared services. A Workspace researcher can also create and use resources, but can only delete or modify resources they have created themselves.</p>"},{"location":"sde/Using-your-workspace/#getting-access-to-the-sde","title":"Getting access to the SDE","text":"<p>As of July 2024, the alpha release of the SDE is only open to specific users, contact us if you think you should be among them. If you have been given access to it, you can log in at https://sde002.uksouth.cloudapp.azure.com/, using your credentials. These will either be: * Your usual Barts Health credentials that you would use to log into your laptop or VDI. Generally your desktop login @ bartshealthnhs.uk and password (e.g.,  SmithJ@bartshealthnhs.uk) * Dedicated credentials to access Precision Medicine related services (e.g., SmithJ@BHPrecisionMedicine.onmicrosoft.com).</p>"},{"location":"sde/Using-your-workspace/#resource-hierarchy-sde-workspace-user","title":"Resource hierarchy: SDE, Workspace, User","text":"<p>Within the SDE, resources for a project are grouped into a Workspace, which has workspace-level resources. These workspace-level resources may also have user-level resources. The terms project and workspace both refer to the same thing as far as the SDE is concerned. You will not have access to any other workspaces and their resources - this is a key feature of the SDE architecture - even if you already have access to that workspace.</p> <p>Some examples of workspace services: * Gitea is a git repository management system. This should be installed once in each workspace to allow all Users in that workspace to use Git to share and manage your code within that workspace. Changes you make in this Git will need to be exported through the Airlock to be used elsewere. * Apache Guacamole is a virtual desktop management system. This allows you to create virtual machines (Linux or Windows), and then get a virtual desktop to access them. Guacamole itself is a project-level resource, so you only need one per workspace, but the VMs created in it are specific to the user, so they are a user-level resource, not shared among multiple users. * MySQL is a standard SQL database service, which, again, probably only needs one deployment per workspace. Each MySQL service can hold multiple databases, each with their own permissions. * Azure Databricks and MLflow are available for machine-learning projects.</p> <p>At the SDE level, the most important shared services are: * Gitea. This shared service is useful for mirroring external Git repositories into the SDE network space so you can access them from within a workspace. Without this, you have no external access to, for example, GitHub. The access is one way, you cannot use this service to push changes back to GitHub. Access to this service can be provided by raising a support issue and identifying the GitHub repositories that need to be mirrored. * Nexus. This is a package mirroring repository from Sonatype, which allows you to download packages from CRAN, PyPi, Ubuntu mirrors, and other sources. If you wish to access a package repository that is not currently mirrored then please raise a support issue.</p> <p>TBD Document how to use these. There are other services available, and new services can be created by building templates for them. If you have a need for a service that's not currently represented, contact us.</p>"},{"location":"sde/Using-your-workspace/#accessing-a-workspace","title":"Accessing a workspace","text":"<p>When you log into the SDE, your initial view will look something like this. There will be a separate workspace for each project you have access to, in this example, there's only one project. </p> <p>Click on the workspace to get the Workspace Overview page. There are other tabs there you can explore, to get more information about your workspace. They're not normally useful during operation, but if you file a bug report or an issue, a screenshot from the Operations tab is often helpful. </p> <p>The workspace currently has no services in it. Using the Create new button on the right will bring up a menu of resources which you can choose from, then you fill in the form with a few parameters, and submit the form. Your workspace resources will then appear here, as you can see in the next screenshot. Here, we see a Gitea service, which is still deploying, and a Guacamole service, which is ready for use.</p> <p>N.B. We don't show the full process for deploying Gitea or Guacamole, since we will do that for you as part of the setup of your workspace. In any case, the process is very similar to that for creating a virtual machine, which is explained in detail below. </p>"},{"location":"sde/Using-your-workspace/#creating-a-virtual-machine","title":"Creating a Virtual Machine","text":"<p>Virtual machines are created via the Guacamole workspace service. From your workspace overview, click on the Guacamole service tile, which takes you to the screen below. Any existing VMs that you have access to will also be listed here, there are none at the moment. </p> <p>Click the Create new button, then select an operating system, Windows or Linux.  </p> <p>The next form is practically identical for Windows or Linux machines, only the dropdown options vary. Fill in the obligatory name and description, select an image (which specifies which types of software the machine will have, what version of the operating system it runs etc), and a size, which specifies the number of CPUs and amount of memory.</p> <p>You cannot, at this time, specify the size of the disk for the virtual machine. That may be added as a feature later on, let us know if you need that capability.</p> <p>Similarly, the options for VM image and size are currently rather limited. We expect to add options in the future for VMs with GPUs etc, and we are aware of the need to customise images for different use-cases. Again, please contact us to let us know your requirements.</p> <p>Leave the Shared storage button selected. This means your VM will have access to storage that exists at the workspace-level, so will persist if you delete your VM. If you only use the VM disk, you will lose everything on it when the VM is destroyed. There's more info on that in the [[Working with data]] page.  </p> <p>Click the Submit button, and after a few seconds, you see something like the next screenshot. You can either click the Go to resource button, or just click directly on the VM tile in the Resources section, to follow the progress of the deployment.</p> <p>The deployment progresses through several stages: * pending means the deployment is queued, but hasn't yet started * deploying means the resource is being constructed * running means the deployment succeeded</p> <p>Other states exist, such as failed, or updating, we don't go into those here.  </p> <p>The Operations tab shows you detail of the deployment steps. The overall progress of the deployment is visible in the top-right in this view. Other buttons are greyed out at this point, and will remain so until the deployment finishes. This typically takes 5-10 minutes, but can take longer sometimes.</p> <p> </p> <p> </p> <p>Now the VM is fully deployed, it's shown as running in the title, and the Update, Delete and Actions buttons are now active. In the Actions menu, there are three options: * Reset password - ignore this, you don't need a password to access the VM. * Start will start the VM if it has been stopped, and * Stop will stop a running VM.</p> <p>You will be charged for your VM all the time it is running. If you know you won't need it for a while (e.g. overnight, weekends, or holidays), it makes sense to Stop it when you don't need it, and Start it again to save your project budget. We envisage adding the ability to auto-shutdown machines which are idle for, say, an hour, but that isn't available yet.</p> <p> </p> <p>Once the VM is fully booted, the Guacamole page will look like this. The VM is shown, with a Connect button, which will launch a new browser window connected to your VM.</p> <p> </p> <p>Clicking the Connect button takes you to your virtual desktop. In this case, it's Ubuntu. You can hover over the icons for help on each.</p> <p> </p> <p>Clicking the Terminal icon brings up a terminal window. When you're finished with your session, just close the browser tab. Reconnecting later will bring it back in the same state.</p> <p> </p>"},{"location":"sde/Using-your-workspace/#working-with-linux-vms","title":"Working with Linux VMs","text":"<ul> <li>Your Linux VM has no direct connectivity to the internet. If you need to update a package, or install new software, you can access various mirrors courtesy of the Nexus shared service, but you won't be able to download software from arbitrary sites.</li> <li>VMs are not backed up. If you delete a VM, everything on the disk is destroyed, with no possibility of recovering it.</li> <li>On the other hand, stopping and restarting the VM from the Action menu does not wipe the contents of your home directory, so is a safe way of saving money when you don't need the VM for a while. However, the /tmp directory is wiped on reboots, so don't store anything there if you want to keep it.</li> <li>The home directory is mounted on the root filesystem, which is not large. If you fill the disk completely, your VM may stop responding, and you may lose access to it, possibly losing your work in the process. For this reason, it's better to work with the shared storage we will configure into your workspace.</li> </ul>"},{"location":"sde/Using-your-workspace/#more-details-about-workspaces","title":"More details about workspaces","text":""},{"location":"sde/Using-your-workspace/#keeping-vms-up-to-date","title":"Keeping VMs up to date","text":"<p>you should make sure you update the OS and software on your VMs regularly. Even though they're contained within a firewall, it's good best practice to make sure you're not running old software. For this reason, we'll be updating the VM templates regularly, and you can take advantage of that by simply deleting your VM and creating a new one.</p> <p>This is the recommended best practice, and it requires that you think carefully about how you customise your machines. If you find that you need to put a lot of effort into customising an image, let us know, we may be able to do this for you, and automate the process.</p>"},{"location":"sde/Using-your-workspace/#considerations-for-workspace-managers","title":"Considerations for workspace managers","text":""},{"location":"sde/Using-your-workspace/#cost-efficiency","title":"Cost efficiency","text":"<p>Workspace Admins are responsible for managing the cost of their workspaces. Once the MVP goes live, users will be expected to pay for their services, until then, the cost will be absorbed by the Precision Medicine team.</p> <p>You can see the cost of your resources on the tile for that resource.  Generally, the most expensive resources are high-level managed functions like AzureML, or some of the larger VM sizes - particularly those with GPUs. Note that the cost displayed on the UI is from the Azure billing API, and may not directly reflect the cost you will be charged, the exact costing model has not yet been worked out. There is overhead for the SDE platform which has to be taken into account, not just the resources deployed into the workspace.</p> <p>User resources (e.g. VMs), workspace resources, and workspaces themselves can all be 'disabled', to save costs. Services that are disabled are hibernated, to the extent possible for that service, to minimise expenditure. You will still be charged for any storage used by a service during hibernation, but not for compute etc. Storage is generally cheap, so its can be ignored for most purposes.</p> <p>Note that disabling a service does not disable services below them. E.g., disabling Guacamole, the virtual desktop service, does not cascade down to disabling the VMs that have been deployed by it, and disabling the workspace does not disable any of the services within it. If you want to disable everything, you have to start from the bottom up, disabling user resources, then workspace services, then the workspace itself.</p> <p>We recommend that you monitor costs of your services in the first days of accessing your workspace, and decide which services to disable/re-enable based on your budget and convenience.</p> <p>We will provide the means to automatically disable VMs after a certain amount of idle-time, though that's not yet available.</p>"},{"location":"sde/Working-with-data/","title":"Working with data","text":"<p>There are two ways to get data into your workspace, and one way to get data out. Data can be provisioned as part of the workspace configuration, following approval by the DAC, or it can be imported/exported via the Airlock mechanism.</p>"},{"location":"sde/Working-with-data/#data-provisioning-via-the-dac","title":"Data provisioning via the DAC","text":"<p>TBC Something here about how the DAC pipeline pushes data into the workspace, where it lands, how the user finds it. Needs to cover both Linux and Windows clients.</p>"},{"location":"sde/Working-with-data/#the-airlock-mechanism","title":"The Airlock mechanism","text":"<p>The Airlock metaphor captures the way users can import their own data into their workspace, or export it from it. There is no direct access to allow data to be moved into/out of the workspace storage, instead, someone must review the data in each case, and approve or deny the request.</p>"},{"location":"sde/Working-with-data/#importing-data","title":"Importing data","text":"<p>For importing, the workflow looks like this: * A researcher creates a request to import data, * Then they upload their data to secure storage, as part of the request process, * Then they submit the request. * An Airlock Manager, who is appointed per project, will review the request, and approve or deny it. * If approved, the researcher will then receive an email with a secure URL to allow them to retrieve data from the workspace, using a VM within the workspace.</p> <p>Note that there are two important restrictions on importing/exporting data: 1. You can only import or export a single file at a time. If you want to transfer multiple files, you can always zip them together, that's fine. 1. The file is currently limited to a maximum size of 2 GB, because of the requirements to scan it for malware with Azure Defender for Cloud. If you have data larger than 2 GB, either break it into several smaller requests, or contact us to find a solution for you.</p>"},{"location":"sde/Working-with-data/#creating-an-airlock-request","title":"Creating an airlock request","text":"<p>As a pre-requisite, you need either to have the Azure CLI installed on your laptop, or to have the Azure Storage Explorer installed. This example shows the workflow using the Storage Explorer, using the CLI is more self-explanatory, assuming you're familiar with it.</p> <p>In your workspace, in the sidebar on the left, there's an Airlock button. Click that, and you'll see the Airlock landing page. In this example, there are no requests yet. Click the New request button to create one.  </p> <p>Choose between an import request and an export request.  </p> <p>Give a name and a justification, then click Create, then confirm that you want to create the request.  </p> <p>Once your request is created, you need to upload the data before you submit the request.</p> <p>A request can only contain a single file, so if you have multiple files, create a tar or zip of them, and upload that. There are two ways to upload your single file, either using a SAS URL, or using the Azure CLI. At this point, the submission form will have refreshed and at the end of the form there are two tabs (SAS URL and CLI) that provide information needed for the next stage.</p> <p>NB: If you click away from the form, it will disappear. In that case, refresh the Airlock landing page, and you'll see your request there, in draft form. Double-click on it, and the form will open again.</p> <p> </p> <p>This example uses the SAS URL and by default the tab provides the information needed for the Azure Storage Explorer tool. The CLI tab provides a command line example you will need to complete if uploading from the command line using the Azure CLI.</p> <p>Now open the Azure Storage Explorer. If this is your first time using it, click the Sign in with Azure tile, and follow the instructions. Then come back and click Attach to a resource,   and then Blob container or directory.  </p> <p>Select Shared access signature URL (SAS), paste in the URL from your request, click Next, then Connect.  </p> <p>Use the Upload button and select your data file for upload. Leave the parameters at their defaults, and complete the upload. The Storage Explorer should then look like this, showing that your file (\"pi.txt\", in this example) has been successfully uploaded. You can also drag-and-drop into the Storage Explorer to upload your file that way.  </p> <p>Go back to the airlock form, or re-open it by double-clicking on your request in the airlock landing page, and click Submit.  </p> <p>An email will be sent to the airlock reviewer, who will then review your data to check it's safe for upload. They will approve or deny your request, and you will be notified by email of the result.  </p>"},{"location":"sde/Working-with-data/#retrieving-the-data-in-a-workspace-vm","title":"Retrieving the data in a workspace VM","text":"<p>If your request has been approved, you can double-click on the request and see the new SAS URL that you can use for downloading the data into your VM. The Azure Storage Explorer (<code>storage-explorer</code> on the Linux command line) is already installed in the workspace VM images (currently only the Data Science VMs), so you can launch it, and use the same method as above for downloading the file. The first time you run storage-explorer you will need to run:</p> <p><code>sudo snap connect storage-explorer:password-manager-service :password-manager-service</code></p> <p></p>"},{"location":"sde/Working-with-data/#exporting-data","title":"Exporting data","text":"<p>The export process is entirely analogous, simply starting from the workspace VM with the upload of a (possibly tarred or zipped) file.</p>"},{"location":"sde/Working-with-data/#shared-storage","title":"Shared Storage","text":"<p>There's also the possibility of using shared storage to share files between multiple VMs. E.g., if you have a Windows VM and a Linux VM, so you can use OS-specific tools, you may want to share your data between those machines.</p> <p>A workspace can be configured with shared storage, or without. By default, we will configure a reasonable amount of shared storage for you, though if you know of specific needs, let us know. Shared storage is valuable because it will outlive the individual VMs, if you create a file on shared storage on a VM, delete the VM, then create a new VM, it will find the file still there.</p> <p>On Windows VMs, the shared storage is be mounted on the <code>Z:</code> drive. On Linux VMs, the shared storage is mounted under the <code>/shared-storage</code> folder.</p> <p>The files in this folder are always owned by the currently logged-in user, whichever OS you're using, even if you're using multiple machines, so there's no concept (yet) of a proper multi-user filesystem.</p>"},{"location":"subdir/test/","title":"This is a test","text":"<p>insert useful documentation here, please...</p>"}]}